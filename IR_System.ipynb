{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Retrieval - A Boolean Retrieval Approach\n",
    "\n",
    "In this notebook a solution for the retrieval of songs based on boolean queries is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/akasnipe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/akasnipe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "## Python version is 3.11.6\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from functools import total_ordering\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains lyrics of songs in the English language, from 1950 to 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/spotify_millsongdata.csv', sep=\",\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only `artist`, `song`  and `text` will be used for the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl   \n",
       "1   ABBA       Andante, Andante   \n",
       "2   ABBA         As Good As New   \n",
       "3   ABBA                   Bang   \n",
       "4   ABBA       Bang-A-Boomerang   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['artist', 'song', 'text']]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IR System\n",
    "\n",
    "The system will be composed of the following classes:\n",
    "* Posting: The class to implement the Posting objects,\n",
    "* Posting List: The class to implement the Posting List objects,\n",
    "* Term: The class to implement the Term objects,\n",
    "* Inverted Index: The class to implement the Inverted Index objects for Boolean Retrieval,\n",
    "* Song: The class to implement the Song objects,\n",
    "* IR System: The \"main\" class that puts everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posting class\n",
    "\n",
    "@total_ordering\n",
    "class Posting:\n",
    "    \n",
    "    # Initializer, takes a document ID as an argument.\n",
    "    def __init__(self, docID):\n",
    "        self._docID = docID\n",
    "    \n",
    "    # Retrieve a document's contents from a corpus using the document ID.\n",
    "    def get_from_corpus(self, corpus):\n",
    "        return corpus[self._docID]\n",
    "    \n",
    "    # Check equality with another Posting, based on document ID.\n",
    "    def __eq__(self, other):\n",
    "        return self._docID == other._docID\n",
    "    \n",
    "    # Check if this Posting has document ID greater than another Posting.\n",
    "    def __gt__(self, other):\n",
    "        return self._docID > other._docID\n",
    "    \n",
    "    # Provide the string representation of the Posting.\n",
    "    def __repr__(self):\n",
    "        return str(self._docID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posting List class\n",
    "\n",
    "class PostingList:\n",
    "\n",
    "    # Initializer, initializes an empty list of postings.\n",
    "    def __init__(self):\n",
    "        self._postings = []\n",
    "    \n",
    "    # Create a PostingList instance with a single Posting from a document ID.\n",
    "    @classmethod\n",
    "    def from_docID(cls, docID):\n",
    "        posting_list = cls()\n",
    "        posting_list._postings = [(Posting(docID))]\n",
    "        return posting_list\n",
    "    \n",
    "    # Create a PostingList instance from an existing list of Postings.\n",
    "    @classmethod\n",
    "    def from_posting_list(cls, postingList):\n",
    "        plist = cls()\n",
    "        plist._postings = postingList\n",
    "        return plist\n",
    "\n",
    "    # Merge another PostingList into this one, avoiding duplicates.\n",
    "    def merge(self, other):\n",
    "        i = 0  # Index for the other PostingList.\n",
    "        last = self._postings[-1]  # The last Posting in the current list.\n",
    "\n",
    "        while (i < len(other._postings) and last == other._postings[i]):\n",
    "            i += 1  # Increment the index if a duplicate is found.\n",
    "        self._postings += other._postings[i:]  # Append the non-duplicate postings from the other list.\n",
    "    \n",
    "    # Compute the intersection of this PostingList with another.\n",
    "    def intersection(self, other):\n",
    "        intersection = []  # Start with an empty list for the intersection.\n",
    "        i = 0  # Index for this PostingList.\n",
    "        j = 0  # Index for the other PostingList.\n",
    "\n",
    "        while (i < len(self._postings) and j < len(other._postings)): # Loop until one of the lists is exhausted.\n",
    "            if (self._postings[i] == other._postings[j]):\n",
    "                intersection.append(self._postings[i]) # If both postings are equal, add to the intersection.\n",
    "                i += 1 # Increment both indexes.\n",
    "                j += 1\n",
    "            # If postings are different, increment the index for the list with the smallest value.\n",
    "            elif (self._postings[i] < other._postings[j]):\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        return PostingList.from_posting_list(intersection)  # Return a new PostingList of the intersection.\n",
    "\n",
    "    # Compute the union of this PostingList with another.\n",
    "    def union(self, other):\n",
    "        union = []  # Start with an empty list for the union.\n",
    "        i = 0  # Index for this PostingList.\n",
    "        j = 0  # Index for the other PostingList.\n",
    "        while (i < len(self._postings) and j < len(other._postings)): # Loop until one of the lists is exhausted.\n",
    "            if (self._postings[i] == other._postings[j]):\n",
    "                union.append(self._postings[i]) # If both postings are equal, add one to the union.\n",
    "                i += 1 # Increment both indexes.\n",
    "                j += 1\n",
    "            # If postings are different, add the posting with the smallest value to the union and increment its index.\n",
    "            elif (self._postings[i] < other._postings[j]):\n",
    "                union.append(self._postings[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                union.append(other._postings[j])\n",
    "                j += 1\n",
    "        # Add any remaining postings from both lists to the union.\n",
    "        for k in range(i, len(self._postings)):\n",
    "            union.append(self._postings[k])\n",
    "        for k in range(j, len(other._postings)):\n",
    "            union.append(other._postings[k])\n",
    "        return PostingList.from_posting_list(union)  # Return a new PostingList of the union.\n",
    "    \n",
    "    # Retrieve the contents of each Posting from a corpus.\n",
    "    def get_from_corpus(self, corpus):\n",
    "        return list(map(lambda x: x.get_from_corpus(corpus), self._postings))\n",
    "    \n",
    "    # Provide the string representation of the PostingList.\n",
    "    def __repr__(self):\n",
    "        return \", \".join(map(str, self._postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term class\n",
    "\n",
    "# Exception class for handling merge operation errors.\n",
    "class ImpossibleMergeError(Exception):\n",
    "    pass\n",
    "\n",
    "# A class that represents a term in a document, along with its posting list.\n",
    "@total_ordering\n",
    "class Term:\n",
    "\n",
    "    # Initializer, takes a term and a document ID as arguments.\n",
    "    def __init__(self, term, docID):\n",
    "        self.term = term\n",
    "        # Initialize posting_list for the term with a PostingList created from the given document ID.\n",
    "        self.posting_list = PostingList.from_docID(docID)\n",
    "\n",
    "    # Merge another Term's posting list into this one if they have the same term.\n",
    "    def merge(self, other):\n",
    "        if (self.term == other.term):\n",
    "            self.posting_list.merge(other.posting_list)\n",
    "        else:\n",
    "            raise ImpossibleMergeError\n",
    "    \n",
    "    # Check equality with another Term.\n",
    "    def __eq__(self, other):\n",
    "        return self.term == other.term\n",
    "    \n",
    "    # Determine if this Term is greater than another.\n",
    "    def __gt__(self, other):\n",
    "        return self.term > other.term\n",
    "    \n",
    "    # Provide the string representation of the Term.\n",
    "    def __repr__(self):\n",
    "        return self.term + \": \" + repr(self.posting_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining the Inverted Index class, let's define functions to perform normalization, stemming and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Word removal, Normalization and Stemming/Lemmatization\n",
    "\n",
    "def remove_stop_words(text):\n",
    "     \n",
    "    # Start from a list containing the tokens in \"text\"\n",
    "    text_list = text.split()\n",
    "\n",
    "    # Filter out stop words\n",
    "    text_list = [word for word in text_list if word not in set(nltk.corpus.stopwords.words('english'))]\n",
    "\n",
    "    # Join the remaining words into a single string\n",
    "    result = \" \".join(text_list)\n",
    "\n",
    "    return result\n",
    "\n",
    "def normalize(text):\n",
    "\n",
    "    # Make a translation table that maps all punctuation characters to None\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    # Apply the translation table to the input string\n",
    "    result = text.translate(translator)\n",
    "\n",
    "    # Converts the text to lowercase.\n",
    "    result = result.lower()\n",
    "\n",
    "    return result\n",
    "\n",
    "def stem(text, type='porter'):\n",
    "        \n",
    "    # Start from a list containing the tokens in \"text\"\n",
    "    stemmed_text = text.split()\n",
    "\n",
    "    # Create a stemmer object\n",
    "    if type == 'porter':\n",
    "        stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    elif type == 'snowball':\n",
    "        stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    else:\n",
    "        raise ValueError('Stemmer type not supported')\n",
    "\n",
    "    # Loop through each word in the text and retrieve the stem\n",
    "    for i in range(len(stemmed_text)):\n",
    "        stemmed_text[i] = stemmer.stem(stemmed_text[i])\n",
    "\n",
    "    # Join the stemmed words into a single string\n",
    "    result = \" \".join(stemmed_text)\n",
    "\n",
    "    return result\n",
    "\n",
    "def lemmatize(text):\n",
    "     # Start from a list containing the tokens in \"text\"\n",
    "        lemmatized_text = text.split()\n",
    "    \n",
    "        # Create a lemmatizer object\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "        # Loop through each word in the text and retrieve the lemma\n",
    "        for i in range(len(lemmatized_text)):\n",
    "            lemmatized_text[i] = lemmatizer.lemmatize(lemmatized_text[i])\n",
    "    \n",
    "        # Join the lemmatized words into a single string\n",
    "        result = \" \".join(lemmatized_text)\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverted Index class\n",
    "\n",
    "class InvertedIndex:\n",
    "    \n",
    "    # Initialize the inverted index with an empty dictionary.\n",
    "    def __init__(self):\n",
    "        self._dictionary = []\n",
    "        \n",
    "    # Create an inverted index from a corpus of documents\n",
    "    ## Argument word_reduction_type enables to choose between stemming and lemmatization\n",
    "    ## Argument stop_words enables to maintain stop words (stop_words=True) or remove them (stop_words=False)\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, word_reduction_type = 'stemming_porter', stop_words = True):\n",
    "        intermediate_dict = {}  # Intermediate dictionary to store the terms and their postings.\n",
    "        for docID, song in enumerate(corpus):\n",
    "            # Remove stop words, normalize and stem/lemmatize\n",
    "            document = song.lyrics\n",
    "            if not stop_words:\n",
    "                document = remove_stop_words(document)\n",
    "            document = normalize(document)\n",
    "            if word_reduction_type == 'stemming_porter':\n",
    "                document = stem(document, type = 'porter')\n",
    "            elif word_reduction_type == 'stemming_snowball':\n",
    "                document = stem(document, type = 'snowball')\n",
    "            elif word_reduction_type == 'lemmatization':\n",
    "                document = lemmatize(document)\n",
    "            tokens = list(document.split()) # Tokenize the document into individual words.\n",
    "            biwords = [tokens[i]+' '+tokens[i+1] for i in range(len(tokens)-1)] # Get all biwords in the document.\n",
    "            for token in tokens:\n",
    "                term = Term(token, docID) # Create a new term with the token and the current document ID.\n",
    "                try: # Try to merge the term with existing one in the intermediate dictionary.\n",
    "                    intermediate_dict[token].merge(term)\n",
    "                except KeyError: # If the term is not already in the dictionary, add it.\n",
    "                    intermediate_dict[token] = term\n",
    "            for biword in biwords:\n",
    "                term = Term(biword, docID) # Create a new term with the biword and the current document ID.\n",
    "                try: # Try to merge the term with existing one in the intermediate dictionary.\n",
    "                    intermediate_dict[biword].merge(term)\n",
    "                except KeyError: # If the term is not already in the dictionary, add it.\n",
    "                    intermediate_dict[biword] = term\n",
    "        idx = cls() # Create a new InvertedIndex instance.\n",
    "        idx._dictionary = sorted(intermediate_dict.values(), key=lambda term: term.term) # Sort the terms in the intermediate dictionary and store them in the index's dictionary.\n",
    "        return idx\n",
    "    \n",
    "    # Retrieve the posting list for a given term.\n",
    "    def __getitem__(self, key):\n",
    "        for term in self._dictionary:\n",
    "            if term.term == key: # If the term matches the key, return its posting list.\n",
    "                return term.posting_list\n",
    "        raise KeyError(\"No song matches the given query.\") # If the term is not in the dictionary, raise a KeyError.\n",
    "    \n",
    "    # Provide a string representation of the inverted index.\n",
    "    def __repr__(self):\n",
    "        return \"A dictionary with \" + str(len(self._dictionary)) + \" terms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Song class\n",
    "\n",
    "# Class to hold the title, author, genre, topic and lyrics of a song\n",
    "class Song:\n",
    "    \n",
    "    # Initializer, initializes the title, author, genre, topic and lyrics attributes.\n",
    "    def __init__(self, title, author, lyrics):\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.lyrics = lyrics\n",
    "        \n",
    "    # Provide the string representation of the Song object.\n",
    "    def __repr__(self):\n",
    "        return \"Title: \" + self.title + \",\\nAuthor: \" + self.author + \"\\n\\n\"\n",
    "    \n",
    "# Get song author, title and lyrics from data\n",
    "def get_songs_data(path):\n",
    "    data = pd.read_csv(path, sep=\",\")\n",
    "    # Remove newline characters from song lyrics\n",
    "    data['text'] = data['text'].replace('\\r\\n',' ', regex=True)\n",
    "    corpus = []\n",
    "    for index, item in data.iterrows():\n",
    "        song = Song(title = item['song'],\n",
    "                    author = item['artist'],\n",
    "                    lyrics = item['text'])\n",
    "        # Add the Song object to the corpus.\n",
    "        corpus.append(song)\n",
    "    # Return the populated list of MovieDescription objects.\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Retrieval (IR) system class\n",
    "\n",
    "class IRsystem:\n",
    "\n",
    "    # Initialize the IR system with a corpus and the inverted index.   \n",
    "    def __init__(self, corpus, index):\n",
    "        self._corpus = corpus\n",
    "        self._index = index\n",
    "    \n",
    "    # Create an IR system instance from a given corpus.\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, word_reduction_type = 'stemming_porter', stop_words=True):\n",
    "        index = InvertedIndex.from_corpus(corpus, word_reduction_type, stop_words)\n",
    "        return cls(corpus, index)\n",
    "    \n",
    "    # Return the posting list of a given posting\n",
    "    def get_posting_list(self, posting):\n",
    "        # Retrieve the posting list from the index.\n",
    "        posting_list = self._index[posting]\n",
    "        # Return the list of documents.\n",
    "        return posting_list.get_from_corpus(self._corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a text query against an IR system.\n",
    "\n",
    "def query(ir, query, word_reduction_type = 'stemming_porter'):\n",
    "    answer = set()\n",
    "    # Split the text query into individual words/biwords.\n",
    "    words = re.split('(AND|OR|NOT)', query)\n",
    "    for i in range(len(words)):\n",
    "        words[i] = words[i].strip()\n",
    "    # Check if the first or the last word is a boolean operator and return an error.\n",
    "    if words[0] in [\"AND\", \"OR\", \"NOT\"] or words[len(words)-1] in [\"AND\", \"OR\", \"NOT\"]:\n",
    "        raise KeyError(\"The first and the last word of the query cannot be a boolean operator.\")\n",
    "    # Normalize and stem/lemmatize the query words/biwords but not the boolean operators.\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in [\"AND\", \"OR\", \"NOT\"]:\n",
    "            if word_reduction_type == 'stemming_porter':                   \n",
    "                words[i] = stem(normalize(words[i]), type = 'porter')\n",
    "            elif word_reduction_type == 'stemming_snowball':\n",
    "                words[i] = stem(normalize(words[i]), type = 'snowball')\n",
    "            elif word_reduction_type == 'lemmatization':\n",
    "                words[i] = lemmatize(normalize(words[i]))\n",
    "    # Retrieve the posting list for the first word/biword from the index.\n",
    "    result = ir.get_posting_list(words[0])\n",
    "    for song in result:\n",
    "        answer.add(song)\n",
    "    # Loop through the remaining words in the query.\n",
    "    for i in range(1, len(words), 2):\n",
    "        # Retrieve the posting lists for the next word from the index.\n",
    "        result = ir.get_posting_list(words[i+1])\n",
    "        # Case AND: Intersect the current answer with the new posting lists.\n",
    "        if words[i] == \"AND\":\n",
    "            answer = answer.intersection(result)\n",
    "        # Case OR: Union the current answer with the new posting lists.\n",
    "        elif words[i] == \"OR\":\n",
    "            answer = answer.union(result)\n",
    "        # Case NOT: Subtract the new posting lists from the current answer.\n",
    "        elif words[i] == \"NOT\":\n",
    "            answer = answer.difference(result)\n",
    "    # Print out each song that matches the query.\n",
    "    ## If no song matches the query, print out a message.\n",
    "    if len(answer) == 0:\n",
    "        raise KeyError(\"No song matches the given query.\")\n",
    "    for song in answer:\n",
    "        print(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the Boolean Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_songs_data(\"data/spotify_millsongdata.csv\")\n",
    "\n",
    "ir = IRsystem.from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the IR system to a file\n",
    "with open('IRSystem/irsystem.pkl', 'wb') as output:\n",
    "    pickle.dump(ir, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IR system from a file\n",
    "with open('IRSystem/irsystem.pkl', 'rb') as input:\n",
    "    ir = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Irresponsible Hate Anthem,\n",
      "Author: Marilyn Manson\n",
      "\n",
      "\n",
      "Title: American Idiot (Greenday Cover),\n",
      "Author: Avril Lavigne\n",
      "\n",
      "\n",
      "Title: Raleigh Soliloquy Pt. Ii,\n",
      "Author: Sublime\n",
      "\n",
      "\n",
      "Title: American Idiot,\n",
      "Author: Green Day\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(ir, \"american AND idiot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Irresponsible Hate Anthem,\n",
      "Author: Marilyn Manson\n",
      "\n",
      "\n",
      "Title: Raleigh Soliloquy Pt. Ii,\n",
      "Author: Sublime\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(ir, \"american AND idiot NOT media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Man Machine,\n",
      "Author: Robbie Williams\n",
      "\n",
      "\n",
      "Title: New Years Eve,\n",
      "Author: Eagles\n",
      "\n",
      "\n",
      "Title: It's Time,\n",
      "Author: Elvis Costello\n",
      "\n",
      "\n",
      "Title: Raleigh Soliloquy Pt. Ii,\n",
      "Author: Sublime\n",
      "\n",
      "\n",
      "Title: American Idiot,\n",
      "Author: Green Day\n",
      "\n",
      "\n",
      "Title: Dancing On A High Wire,\n",
      "Author: Alan Parsons Project\n",
      "\n",
      "\n",
      "Title: On The Nickel,\n",
      "Author: Tom Waits\n",
      "\n",
      "\n",
      "Title: I'm So Sick Of You,\n",
      "Author: Weird Al Yankovic\n",
      "\n",
      "\n",
      "Title: Throw It Up,\n",
      "Author: Yelawolf\n",
      "\n",
      "\n",
      "Title: American Idiot (Greenday Cover),\n",
      "Author: Avril Lavigne\n",
      "\n",
      "\n",
      "Title: Main Chick,\n",
      "Author: Chris Brown\n",
      "\n",
      "\n",
      "Title: Irresponsible Hate Anthem,\n",
      "Author: Marilyn Manson\n",
      "\n",
      "\n",
      "Title: Waiting For The Punchline,\n",
      "Author: Extreme\n",
      "\n",
      "\n",
      "Title: Liar's Bar,\n",
      "Author: Beautiful South\n",
      "\n",
      "\n",
      "Title: Up In Flames,\n",
      "Author: Nicki Minaj\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(ir, \"american AND idiot OR punchline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
