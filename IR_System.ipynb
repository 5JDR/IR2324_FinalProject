{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Retrieval - A Boolean Retrieval Approach\n",
    "\n",
    "In this notebook a solution for the retrieval of songs based on boolean queries is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/akasnipe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/akasnipe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "## Python version is 3.11.6\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from functools import total_ordering\n",
    "import re\n",
    "import _pickle as pickle # cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains lyrics of songs in the English language, from 1950 to 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/spotify_millsongdata.csv', sep=\",\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only `artist`, `song`  and `text` will be used for the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl   \n",
       "1   ABBA       Andante, Andante   \n",
       "2   ABBA         As Good As New   \n",
       "3   ABBA                   Bang   \n",
       "4   ABBA       Bang-A-Boomerang   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['artist', 'song', 'text']]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IR System\n",
    "\n",
    "The system will be composed of the following classes:\n",
    "* Posting: The class to implement the Posting objects,\n",
    "* Posting List: The class to implement the Posting List objects,\n",
    "* Term: The class to implement the Term objects,\n",
    "* Inverted Index: The class to implement the Inverted Index objects for Boolean Retrieval,\n",
    "* Song: The class to implement the Song objects,\n",
    "* IR System: The \"main\" class that puts everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posting class\n",
    "\n",
    "@total_ordering\n",
    "class Posting:\n",
    "    \n",
    "    # Initializer, takes a document ID as an argument.\n",
    "    def __init__(self, docID):\n",
    "        self._docID = docID\n",
    "    \n",
    "    # Retrieve a document's contents from a corpus using the document ID.\n",
    "    def get_from_corpus(self, corpus):\n",
    "        return corpus[self._docID]\n",
    "    \n",
    "    # Check equality with another Posting, based on document ID.\n",
    "    def __eq__(self, other):\n",
    "        return self._docID == other._docID\n",
    "    \n",
    "    # Check if this Posting has document ID greater than another Posting.\n",
    "    def __gt__(self, other):\n",
    "        return self._docID > other._docID\n",
    "    \n",
    "    # Provide the string representation of the Posting.\n",
    "    def __repr__(self):\n",
    "        return str(self._docID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posting List class\n",
    "\n",
    "class PostingList:\n",
    "\n",
    "    # Initializer, initializes an empty list of postings.\n",
    "    def __init__(self):\n",
    "        self._postings = []\n",
    "    \n",
    "    # Create a PostingList instance with a single Posting from a document ID.\n",
    "    @classmethod\n",
    "    def from_docID(cls, docID):\n",
    "        posting_list = cls()\n",
    "        posting_list._postings = [(Posting(docID))]\n",
    "        return posting_list\n",
    "    \n",
    "    # Create a PostingList instance from an existing list of Postings.\n",
    "    @classmethod\n",
    "    def from_posting_list(cls, postingList):\n",
    "        plist = cls()\n",
    "        plist._postings = postingList\n",
    "        return plist\n",
    "\n",
    "    # Merge another PostingList into this one, avoiding duplicates.\n",
    "    def merge(self, other):\n",
    "        i = 0  # Index for the other PostingList.\n",
    "        last = self._postings[-1]  # The last Posting in the current list.\n",
    "\n",
    "        while (i < len(other._postings) and last == other._postings[i]):\n",
    "            i += 1  # Increment the index if a duplicate is found.\n",
    "        self._postings += other._postings[i:]  # Append the non-duplicate postings from the other list.\n",
    "    \n",
    "    # Retrieve the contents of each Posting from a corpus.\n",
    "    def get_from_corpus(self, corpus):\n",
    "        return list(map(lambda x: x.get_from_corpus(corpus), self._postings))\n",
    "    \n",
    "    # Provide the string representation of the PostingList.\n",
    "    def __repr__(self):\n",
    "        return \", \".join(map(str, self._postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term class\n",
    "\n",
    "# Exception class for handling merge operation errors.\n",
    "class ImpossibleMergeError(Exception):\n",
    "    pass\n",
    "\n",
    "# A class that represents a term in a document, along with its posting list.\n",
    "@total_ordering\n",
    "class Term:\n",
    "\n",
    "    # Initializer, takes a term and a document ID as arguments.\n",
    "    def __init__(self, term, docID):\n",
    "        self.term = term\n",
    "        # Initialize posting_list for the term with a PostingList created from the given document ID.\n",
    "        self.posting_list = PostingList.from_docID(docID)\n",
    "\n",
    "    # Merge another Term's posting list into this one if they have the same term.\n",
    "    def merge(self, other):\n",
    "        if (self.term == other.term):\n",
    "            self.posting_list.merge(other.posting_list)\n",
    "        else:\n",
    "            raise ImpossibleMergeError\n",
    "    \n",
    "    # Check equality with another Term.\n",
    "    def __eq__(self, other):\n",
    "        return self.term == other.term\n",
    "    \n",
    "    # Determine if this Term is greater than another.\n",
    "    def __gt__(self, other):\n",
    "        return self.term > other.term\n",
    "    \n",
    "    # Provide the string representation of the Term.\n",
    "    def __repr__(self):\n",
    "        return self.term + \": \" + repr(self.posting_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining the Inverted Index class, let's define functions to perform normalization, stemming and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Word removal, Normalization and Stemming/Lemmatization\n",
    "\n",
    "def remove_stop_words(text):\n",
    "     \n",
    "    # Start from a list containing the tokens in \"text\"\n",
    "    text_list = text.split()\n",
    "\n",
    "    # Filter out stop words\n",
    "    text_list = [word for word in text_list if word not in set(nltk.corpus.stopwords.words('english'))]\n",
    "\n",
    "    # Join the remaining words into a single string\n",
    "    result = \" \".join(text_list)\n",
    "\n",
    "    return result\n",
    "\n",
    "def normalize(text):\n",
    "\n",
    "    # Make a translation table that maps all punctuation characters to None\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    # Apply the translation table to the input string\n",
    "    result = text.translate(translator)\n",
    "\n",
    "    # Converts the text to lowercase.\n",
    "    result = result.lower()\n",
    "\n",
    "    return result\n",
    "\n",
    "def stem(text, type='porter'):\n",
    "        \n",
    "    # Start from a list containing the tokens in \"text\"\n",
    "    stemmed_text = text.split()\n",
    "\n",
    "    # Create a stemmer object\n",
    "    if type == 'porter':\n",
    "        stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    elif type == 'snowball':\n",
    "        stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    else:\n",
    "        raise ValueError('Stemmer type not supported')\n",
    "\n",
    "    # Loop through each word in the text and retrieve the stem\n",
    "    for i in range(len(stemmed_text)):\n",
    "        stemmed_text[i] = stemmer.stem(stemmed_text[i])\n",
    "\n",
    "    # Join the stemmed words into a single string\n",
    "    result = \" \".join(stemmed_text)\n",
    "\n",
    "    return result\n",
    "\n",
    "def lemmatize(text):\n",
    "     # Start from a list containing the tokens in \"text\"\n",
    "        lemmatized_text = text.split()\n",
    "    \n",
    "        # Create a lemmatizer object\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "        # Loop through each word in the text and retrieve the lemma\n",
    "        for i in range(len(lemmatized_text)):\n",
    "            lemmatized_text[i] = lemmatizer.lemmatize(lemmatized_text[i])\n",
    "    \n",
    "        # Join the lemmatized words into a single string\n",
    "        result = \" \".join(lemmatized_text)\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverted Index class\n",
    "\n",
    "class InvertedIndex:\n",
    "    \n",
    "    # Initialize the inverted index with an empty dictionary.\n",
    "    def __init__(self):\n",
    "        self._dictionary = []\n",
    "        \n",
    "    # Create an inverted index from a corpus of documents\n",
    "    ## Argument word_reduction_type enables to choose between stemming and lemmatization\n",
    "    ## Argument stop_words enables to maintain stop words (stop_words=True) or remove them (stop_words=False)\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, word_reduction_type = 'stemming_porter', stop_words = True):\n",
    "        intermediate_dict = {}  # Intermediate dictionary to store the terms and their postings.\n",
    "        for docID, song in enumerate(corpus):\n",
    "            # Remove stop words, normalize and stem/lemmatize\n",
    "            document = song.lyrics\n",
    "            document = normalize(document)\n",
    "            if not stop_words:\n",
    "                document = remove_stop_words(document)\n",
    "            if word_reduction_type == 'stemming_porter':\n",
    "                document = stem(document, type = 'porter')\n",
    "            elif word_reduction_type == 'stemming_snowball':\n",
    "                document = stem(document, type = 'snowball')\n",
    "            elif word_reduction_type == 'lemmatization':\n",
    "                document = lemmatize(document)\n",
    "            tokens_list = document.split() # Tokenize the document into individual words.\n",
    "            tokens = set(tokens_list) # Remove duplicates         \n",
    "            biwords = set([tokens_list[i]+' '+tokens_list[i+1] for i in range(len(tokens_list)-1)]) # Get all biwords in the document, remove duplicates.\n",
    "            for token in tokens:\n",
    "                term = Term(token, docID) # Create a new term with the token and the current document ID.\n",
    "                try: # Try to merge the term with existing one in the intermediate dictionary.\n",
    "                    intermediate_dict[token].merge(term)\n",
    "                except KeyError: # If the term is not already in the dictionary, add it.\n",
    "                    intermediate_dict[token] = term\n",
    "            for biword in biwords:\n",
    "                term = Term(biword, docID) # Create a new term with the biword and the current document ID.\n",
    "                try: # Try to merge the term with existing one in the intermediate dictionary.\n",
    "                    intermediate_dict[biword].merge(term)\n",
    "                except KeyError: # If the term is not already in the dictionary, add it.\n",
    "                    intermediate_dict[biword] = term\n",
    "        idx = cls() # Create a new InvertedIndex instance.\n",
    "        idx._dictionary = sorted(intermediate_dict.values(), key=lambda term: term.term) # Sort the terms in the intermediate dictionary and store them in the index's dictionary.\n",
    "        return idx\n",
    "    \n",
    "    # Retrieve the posting list for a given term.\n",
    "    def __getitem__(self, key):\n",
    "        for term in self._dictionary:\n",
    "            if term.term == key: # If the term matches the key, return its posting list.\n",
    "                return term.posting_list\n",
    "        raise KeyError(\"No song matches the given query.\") # If the term is not in the dictionary, raise a KeyError.\n",
    "    \n",
    "    # Provide a string representation of the inverted index.\n",
    "    def __repr__(self):\n",
    "        return \"A dictionary with \" + str(len(self._dictionary)) + \" terms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Song class\n",
    "\n",
    "# Class to hold the title, author, genre, topic and lyrics of a song\n",
    "class Song:\n",
    "    \n",
    "    # Initializer, initializes the title, author, genre, topic and lyrics attributes.\n",
    "    def __init__(self, title, author, lyrics):\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.lyrics = lyrics\n",
    "        \n",
    "    # Provide the string representation of the Song object.\n",
    "    def __repr__(self):\n",
    "        return \"Title: \" + self.title + \",\\nAuthor: \" + self.author + \"\\n\\n\"\n",
    "    \n",
    "# Get song author, title and lyrics from data\n",
    "def get_songs_data(path):\n",
    "    data = pd.read_csv(path, sep=\",\")\n",
    "    # Remove newline characters from song lyrics\n",
    "    data['text'] = data['text'].replace('\\r\\n',' ', regex=True)\n",
    "    corpus = []\n",
    "    for index, item in data.iterrows():\n",
    "        song = Song(title = item['song'],\n",
    "                    author = item['artist'],\n",
    "                    lyrics = item['text'])\n",
    "        # Add the Song object to the corpus.\n",
    "        corpus.append(song)\n",
    "    # Return the populated list of MovieDescription objects.\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Retrieval (IR) system class\n",
    "\n",
    "class IRsystem:\n",
    "\n",
    "    # Initialize the IR system with a corpus and the inverted index.   \n",
    "    def __init__(self, corpus, index):\n",
    "        self._corpus = corpus\n",
    "        self._index = index\n",
    "    \n",
    "    # Create an IR system instance from a given corpus.\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, word_reduction_type = 'stemming_porter', stop_words=True):\n",
    "        index = InvertedIndex.from_corpus(corpus, word_reduction_type, stop_words)\n",
    "        return cls(corpus, index)\n",
    "    \n",
    "    # Return the posting list of a given posting\n",
    "    def get_posting_list(self, posting):\n",
    "        # Retrieve the posting list from the index.\n",
    "        posting_list = self._index[posting]\n",
    "        # Return the list of documents.\n",
    "        return posting_list.get_from_corpus(self._corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a text query against an IR system.\n",
    "\n",
    "def query(ir, query, word_reduction_type = 'stemming_porter', stopwords = True, _print = True):\n",
    "    answer = set()\n",
    "    # Split the text query into individual words/biwords.\n",
    "    words = re.split('(AND|OR|NOT)', query)\n",
    "    for i in range(len(words)):\n",
    "        words[i] = words[i].strip()\n",
    "    # Check if the first or the last word is a boolean operator and return an error.\n",
    "    if words[0] in [\"AND\", \"OR\", \"NOT\"] or words[len(words)-1] in [\"AND\", \"OR\", \"NOT\"]:\n",
    "        raise KeyError(\"The first and the last word of the query cannot be a boolean operator.\")\n",
    "    # Normalize, remove stopwords and stem/lemmatize the query words/biwords but not the boolean operators.\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in [\"AND\", \"OR\", \"NOT\"]:\n",
    "            if word_reduction_type == 'stemming_porter':    \n",
    "                words[i] = normalize(words[i])\n",
    "                if not stopwords:\n",
    "                    words[i] = remove_stop_words(words[i])               \n",
    "                words[i] = stem(words[i], type = 'porter')\n",
    "            elif word_reduction_type == 'stemming_snowball':\n",
    "                words[i] = normalize(words[i])\n",
    "                if not stopwords:\n",
    "                    words[i] = remove_stop_words(words[i]) \n",
    "                words[i] = stem(words[i], type = 'snowball')\n",
    "            elif word_reduction_type == 'lemmatization':\n",
    "                words[i] = normalize(words[i])\n",
    "                if not stopwords:\n",
    "                    words[i] = remove_stop_words(words[i]) \n",
    "                words[i] = lemmatize(words[i])\n",
    "    # Retrieve the posting list for the first word/biword from the index.\n",
    "    result = ir.get_posting_list(words[0])\n",
    "    for song in result:\n",
    "        answer.add(song)\n",
    "    # Loop through the remaining words in the query.\n",
    "    for i in range(1, len(words), 2):\n",
    "        # Retrieve the posting lists for the next word from the index.\n",
    "        result = ir.get_posting_list(words[i+1])\n",
    "        # Case AND: Intersect the current answer with the new posting lists.\n",
    "        if words[i] == \"AND\":\n",
    "            answer = answer.intersection(result)\n",
    "        # Case OR: Unite the current answer with the new posting lists.\n",
    "        elif words[i] == \"OR\":\n",
    "            answer = answer.union(result)\n",
    "        # Case NOT: Subtract the new posting lists from the current answer.\n",
    "        elif words[i] == \"NOT\":\n",
    "            answer = answer.difference(result)\n",
    "    # Print out each song that matches the query.\n",
    "    ## If no song matches the query, print out a message.\n",
    "    if len(answer) == 0:\n",
    "        raise KeyError(\"No song matches the given query.\")\n",
    "    if _print:\n",
    "        for song in answer:\n",
    "            print(song)\n",
    "    else:\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the Boolean Retrieval System with different parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_songs_data(\"data/spotify_millsongdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save on disk the IR System with different parametrizations\n",
    "\n",
    "for word_reduction_type in ['stemming_porter', 'stemming_snowball', 'lemmatization']:\n",
    "    for stopwords in [True, False]:\n",
    "        ir = IRsystem.from_corpus(corpus, word_reduction_type, stopwords)\n",
    "        filename = 'IRSystem/ir_' + word_reduction_type + '_with_stopwords' + '.pkl'\n",
    "        if not stopwords:\n",
    "            filename = 'IRSystem/ir_' + word_reduction_type + '_without_stopwords' + '.pkl'\n",
    "        with open(filename, 'wb') as output:\n",
    "            pickle.dump(ir, output, protocol=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, there will be a comparison between the time needed to generate the IR system and the time needed to load it from disk. The times may vary from machine to machine, but the order of magnitude should not change much.\n",
    "\n",
    "Generating the IR system and dumping it on disk takes approximately 70 minutes, this means that each IR system took approximately 10-12 minutes to be generated and saved.\n",
    "\n",
    "During experimentation, the generation of a single IR system took approximately 6 minutes (with different times for different parametrizations), and also the dumping on disk took almost the same amount of time.\n",
    "\n",
    "Suggestion: free up some RAM before running the following cells. Restarting the kernel and running all the cells from 1 to 11 should be enough.\n",
    "\n",
    "Let's now test the IR system in its default parametrization (porter stemming, with stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IR system from a file\n",
    "\n",
    "with open('IRSystem/ir_stemming_porter_with_stopwords.pkl', 'rb') as handle:\n",
    "    ir = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading an Inverted Index from disk takes approximately 1 minute, which is an improvement with respect to the time needed to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: American Idiot,\n",
      "Author: Green Day\n",
      "\n",
      "\n",
      "Title: American Idiot (Greenday Cover),\n",
      "Author: Avril Lavigne\n",
      "\n",
      "\n",
      "Title: Raleigh Soliloquy Pt. Ii,\n",
      "Author: Sublime\n",
      "\n",
      "\n",
      "Title: Irresponsible Hate Anthem,\n",
      "Author: Marilyn Manson\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(ir, \"american AND idiot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the IR system returns (also) the song American Idiot by Green Day, which is expected from the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Raleigh Soliloquy Pt. Ii,\n",
      "Author: Sublime\n",
      "\n",
      "\n",
      "Title: Irresponsible Hate Anthem,\n",
      "Author: Marilyn Manson\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(ir, \"american AND idiot NOT media\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the \"NOT media\" part is added to the query, the IR system returns the same results as before, but without the song American Idiot by Green Day (and the Avril Lavigne cover), since the word \"media\" is present in the lyrics of this song, but not in the lyrics of the other two songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: American Idiot,\n",
      "Author: Green Day\n",
      "\n",
      "\n",
      "Title: All The Small Things (Blink 182 Cover),\n",
      "Author: Avril Lavigne\n",
      "\n",
      "\n",
      "Title: American Idiot (Greenday Cover),\n",
      "Author: Avril Lavigne\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(ir, \"american idiot OR work sucks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also phrasal queries work as expected, the IR system returned the songs American Idiot (the only song where the phrase \"american idiot\" is present) and All The Small Things (the only song where the phrase \"work sucks\" is present).\n",
    "\n",
    "Let's now evaluate the IR system, with the different parametrizations, and analyze the results.\n",
    "\n",
    "### IR System Evaluation\n",
    "\n",
    "Since the chosen dataset only contains the documents used by the IR system, there is no reference dataset for the evaluation.\n",
    "\n",
    "For this reason, a simple evaluation procedure has been implemented, which consists in the following steps:\n",
    "1. Sample some songs from the dataset.\n",
    "    - For each song, compute the 5 most frequent words/bigrams in the lyrics (including and excluding stopwords, in order to test the IR system with different parametrizations).\n",
    "    - Use these words for the queries, **A document is considered relevant if it has the word among the 5 most frequent ones in its lyrics**. Other techniques could have been used to determine the relevance of a document, but this one has been thought as simple and effective enough in this tricky situation.\n",
    "2. Pick 3 words/bigrams at random from the previous result (`word1`, `word2`, `word3`).\n",
    "3. Loop N times:\n",
    "    Randomly generate K queries, with K random (ranging from 1 to 6), each one having one of the following 6 structures (again, chosen at random):\n",
    "    - Simple `AND` query: \"word1 AND word2\"\n",
    "    - Simple `OR` query: \"word1 OR word2\"\n",
    "    - Simple `NOT` query: \"NOT word1\"\n",
    "    - Complex `AND NOT` query: \"word1 AND word2 NOT word3\"\n",
    "    - Complex `OR NOT` query: \"word1 OR word2 NOT word3\"\n",
    "    - Simple 1 word query: \"word1\"\n",
    "    Each query is associated to the relevant documents for that query.\n",
    "3. Run the IR system on the generated queries, compute the precision and recall for each query, and average them over all the queries.\n",
    "4. Analyze the results.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample N songs from the dataset\n",
    "\n",
    "sample = data.sample(n=500, replace=False)\n",
    "\n",
    "# For each lyrics, compute the frequency of words and biwords, maintain only the 5 most frequent words/biwords\n",
    "\n",
    "def get_top5_frequencies(corpus, stop_words = True):\n",
    "    frequencies = pd.DataFrame(columns=['songID', 'song', 'author', 'word', 'frequency'])\n",
    "    # Loop through each song in the corpus and retrieve the frequencies of words and biwords\n",
    "    for index, item in corpus.iterrows():\n",
    "        song = item['song']\n",
    "        author = item['artist']\n",
    "        lyrics = item['text']\n",
    "        if not stop_words:\n",
    "                lyrics = normalize(lyrics)\n",
    "                lyrics = remove_stop_words(lyrics)\n",
    "        tokens_list = lyrics.split() # Tokenize the lyrics into individual words.\n",
    "        if not stop_words:\n",
    "            tokens_list = [token for token in tokens_list if len(token) > 3] # Exclude tokens that have less than 4 characters. (e.g. 'she', 'im', 'ive', etc.)\n",
    "        tokens = set(tokens_list) # Remove duplicates.\n",
    "        biwords_list = [tokens_list[i]+' '+tokens_list[i+1] for i in range(len(tokens_list)-1)] # Get all biwords in the document\n",
    "        biwords = set(biwords_list) # Remove duplicates.           \n",
    "        tmp_frequencies = pd.DataFrame(columns=['songID', 'song', 'author', 'word', 'frequency'])\n",
    "        for token in tokens:\n",
    "            tmp_frequencies = pd.concat([tmp_frequencies, pd.DataFrame({'songID': index, 'song': song, 'author': author, 'word': token, 'frequency': tokens_list.count(token)}, index=[0])], ignore_index=True)\n",
    "        for biword in biwords:\n",
    "            tmp_frequencies = pd.concat([tmp_frequencies, pd.DataFrame({'songID': index, 'song': song, 'author': author, 'word': biword, 'frequency': biwords_list.count(biword)}, index=[0])], ignore_index=True)\n",
    "        # Maintain only the 5 most frequent words/biwords\n",
    "        tmp_frequencies = tmp_frequencies.sort_values(by=['frequency'], ascending=False).head(5)\n",
    "        # Append the frequencies of words and biwords to the dataframe\n",
    "        frequencies = pd.concat([frequencies, tmp_frequencies], ignore_index=True)\n",
    "    return frequencies\n",
    "\n",
    "frequencies_with_stopwords = get_top5_frequencies(sample)\n",
    "frequencies_without_stopwords = get_top5_frequencies(sample, stop_words=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been decided to sample 500 songs from the dataset, after experimentation this has been chosen as a compromise between the variety of songs used for evaluation and the time needed to execute the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of queries\n",
    "\n",
    "def make_queries(data, n=350):\n",
    "    queries = pd.DataFrame(columns=['query_type', 'query', 'relevant_song_title', 'relevant_song_author'])\n",
    "\n",
    "    for i in range(n):\n",
    "        words = np.random.choice(data['word'].unique(), 3, replace=False)\n",
    "        word1 = words[0]\n",
    "        word2 = words[1]\n",
    "        word3 = words[2]\n",
    "        # Create a random number of queries (from 1 to 5)\n",
    "        combinations = []\n",
    "        n_queries = np.random.randint(1, 6)\n",
    "        if n_queries == 5:\n",
    "            combinations = range(1, 6)\n",
    "        else:\n",
    "            combinations = np.random.choice(range(1, 7), n_queries, replace=False)\n",
    "        for combination in combinations:\n",
    "            match combination:\n",
    "                case 1: # Simple AND query\n",
    "                    # Find all songs where word1 and word2 are among the 5 most frequent words\n",
    "                    relevant_songIDs = pd.merge(data[data['word'] == word1], data[data['word'] == word2], on=['songID'], how='inner')['songID']\n",
    "                    relevant_song_titles = []\n",
    "                    relevant_song_authors = []\n",
    "                    if len(relevant_song_titles) == 0:\n",
    "                        relevant_song_titles = [pd.NA]\n",
    "                        relevant_song_authors = [pd.NA]\n",
    "                    else:\n",
    "                        relevant_song_titles = data[data['songID'].isin(relevant_songIDs)]['song'].values\n",
    "                        relevant_song_authors = data[data['songID'].isin(relevant_songIDs)]['author'].values\n",
    "                    for k in range(len(relevant_song_titles)):\n",
    "                        queries = pd.concat([queries, pd.DataFrame({'query_type': 1, 'query': word1 + ' AND ' + word2, 'relevant_song_title': relevant_song_titles[k], 'relevant_song_author': relevant_song_authors[k]}, index=[0])], ignore_index=True)\n",
    "                    queries = queries.drop_duplicates() # Remove duplicates\n",
    "                case 2: # Simple OR query\n",
    "                    # Find all songs where word1 or word2 are among the 5 most frequent words\n",
    "                    relevant_song_titles = data[(data['word'] == word1) | (data['word'] == word2)]['song'].values\n",
    "                    relevant_song_authors = data[(data['word'] == word1) | (data['word'] == word2)]['author'].values\n",
    "                    if len(relevant_song_titles) == 0:\n",
    "                        relevant_song_titles = [pd.NA]\n",
    "                        relevant_song_authors = [pd.NA]\n",
    "                    for k in range(len(relevant_song_titles)):\n",
    "                        queries = pd.concat([queries, pd.DataFrame({'query_type': 2, 'query': word1 + ' OR ' + word2, 'relevant_song_title': relevant_song_titles[k], 'relevant_song_author': relevant_song_authors[k]}, index=[0])], ignore_index=True)\n",
    "                    queries = queries.drop_duplicates() # Remove duplicates\n",
    "                case 3: # Simple NOT query\n",
    "                    # Find all songs where word1 is among the 5 most frequent words, but word2 is not\n",
    "                    good = data[data['word'] == word1]['songID'].values\n",
    "                    bad = data[data['word'] == word2]['songID'].values\n",
    "                    relevant_song_titles = []\n",
    "                    relevant_song_authors = []\n",
    "                    for songID in good:\n",
    "                        if songID not in bad:\n",
    "                            relevant_song_titles.append(data[data['songID'] == songID]['song'].iloc[0])\n",
    "                            relevant_song_authors.append(data[data['songID'] == songID]['author'].iloc[0])\n",
    "                    if len(relevant_song_titles) == 0:\n",
    "                        relevant_song_titles = [pd.NA]\n",
    "                        relevant_song_authors = [pd.NA]\n",
    "                    for k in range(len(relevant_song_titles)):\n",
    "                        queries = pd.concat([queries, pd.DataFrame({'query_type': 3, 'query': word1 + ' NOT ' + word2, 'relevant_song_title': relevant_song_titles[k], 'relevant_song_author': relevant_song_authors[k]}, index=[0])], ignore_index=True)\n",
    "                    queries = queries.drop_duplicates() # Remove duplicates\n",
    "                case 4: # Complex AND NOT query\n",
    "                    # Find all songs where word1 and word2 are among the 5 most frequent words, but word3 is not\n",
    "                    good = pd.merge(data[data['word'] == word1], data[data['word'] == word2], on=['songID'], how='inner')['songID']\n",
    "                    bad = data[data['word'] == word3]['songID'].values\n",
    "                    relevant_song_titles = []\n",
    "                    relevant_song_authors = []\n",
    "                    if len(good) == 0:\n",
    "                        relevant_song_titles = [pd.NA]\n",
    "                        relevant_song_authors = [pd.NA]\n",
    "                    else:\n",
    "                        for songID in good:\n",
    "                            if songID not in bad:\n",
    "                                relevant_song_titles.append(data[data['songID'] == songID]['song'].iloc[0])\n",
    "                                relevant_song_authors.append(data[data['songID'] == songID]['author'].iloc[0])\n",
    "                    for k in range(len(relevant_song_titles)):\n",
    "                        queries = pd.concat([queries, pd.DataFrame({'query_type': 4, 'query': word1 + ' AND ' + word2 + ' NOT ' + word3, 'relevant_song_title': relevant_song_titles[k], 'relevant_song_author': relevant_song_authors[k]}, index=[0])], ignore_index=True)\n",
    "                    queries = queries.drop_duplicates()\n",
    "                case 5: # Complex OR NOT query\n",
    "                    # Find all songs where word1 or word2 are among the 5 most frequent words, but word3 is not\n",
    "                    good = data[(data['word'] == word1) | (data['word'] == word2)]['songID'].values\n",
    "                    bad = data[data['word'] == word3]['songID'].values\n",
    "                    relevant_song_titles = []\n",
    "                    relevant_song_authors = []\n",
    "                    for songID in good:\n",
    "                        if songID not in bad:\n",
    "                            relevant_song_titles.append(data[data['songID'] == songID]['song'].iloc[0])\n",
    "                            relevant_song_authors.append(data[data['songID'] == songID]['author'].iloc[0])\n",
    "                    if len(relevant_song_titles) == 0:\n",
    "                        relevant_song_titles = [pd.NA]\n",
    "                        relevant_song_authors = [pd.NA]\n",
    "                    for k in range(len(relevant_song_titles)):\n",
    "                        queries = pd.concat([queries, pd.DataFrame({'query_type': 5, 'query': word1 + ' OR ' + word2 + ' NOT ' + word3, 'relevant_song_title': relevant_song_titles[k], 'relevant_song_author': relevant_song_authors[k]}, index=[0])], ignore_index=True)\n",
    "                    queries = queries.drop_duplicates() # Remove duplicates\n",
    "                case 6: # Single word/biword query\n",
    "                    # Find all songs where word1 is among the 5 most frequent words\n",
    "                    relevant_song_titles = data[data['word'] == word1]['song'].values\n",
    "                    relevant_song_authors = data[data['word'] == word1]['author'].values\n",
    "                    for k in range(len(relevant_song_titles)):\n",
    "                        queries = pd.concat([queries, pd.DataFrame({'query_type': 6, 'query': word1, 'relevant_song_title': relevant_song_titles[k], 'relevant_song_author': relevant_song_authors[k]}, index=[0])], ignore_index=True)\n",
    "                    queries = queries.drop_duplicates()\n",
    "    return queries\n",
    "\n",
    "queries_with_stopwords = make_queries(frequencies_with_stopwords)\n",
    "queries_without_stopwords = make_queries(frequencies_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been decided to loop the query generator 350 times, again after some experimentation, in order to end up having a few thousand queries to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision and recall\n",
    "\n",
    "def evaluate(ir, queries):\n",
    "\n",
    "    evaluation = pd.DataFrame(columns=['query_type', 'precision', 'recall'])\n",
    "\n",
    "    # Compute precision and recall for each query\n",
    "    for _query in queries['query'].unique():\n",
    "        relevant_titles = queries[queries['query'] == _query]['relevant_song_title'].values\n",
    "        relevant_authors = queries[queries['query'] == _query]['relevant_song_author'].values\n",
    "        relevant_songs = pd.DataFrame(columns=['title', 'author'])\n",
    "        for i in range(len(relevant_titles)):\n",
    "            relevant_songs = pd.concat([relevant_songs, pd.DataFrame({'title': relevant_titles[i], 'author': relevant_authors[i]}, index=[0])], ignore_index=True)\n",
    "        try:\n",
    "            retrieved_songs = pd.DataFrame({'title': [song.title for song in query(ir, _query, _print=False)], 'author': [song.author for song in query(ir, _query, _print=False)]})\n",
    "            # Compute precision and recall\n",
    "            precision = len(pd.merge(relevant_songs, retrieved_songs, on=['title', 'author'], how='inner')) / len(retrieved_songs)\n",
    "            recall = len(pd.merge(relevant_songs, retrieved_songs, on=['title', 'author'], how='inner')) / len(relevant_songs)\n",
    "            evaluation = pd.concat([evaluation, pd.DataFrame({'query_type': queries[queries['query'] == _query]['query_type'].values[0], 'precision': precision, 'recall': recall}, index=[0])], ignore_index=True)      \n",
    "        except KeyError:\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "            evaluation = pd.concat([evaluation, pd.DataFrame({'query_type': queries[queries['query'] == _query]['query_type'].values[0], 'precision': precision, 'recall': recall}, index=[0])], ignore_index=True)\n",
    "            continue\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the evaluation of all the previously generated IR systems will be performed.\n",
    "\n",
    "Suggestion: free up some RAM before running the following cells. Restarting the kernel and running all the cells from 1 to 11, and from 15 to 17 should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the IR system with different parametrizations\n",
    "\n",
    "results = pd.DataFrame(columns=['word_reduction_type', 'stopwords', 'query_type', 'precision', 'recall'])\n",
    "\n",
    "for word_reduction_type in ['stemming_porter', 'stemming_snowball', 'lemmatization']:\n",
    "    for stopwords in [True, False]:\n",
    "        if stopwords:\n",
    "            with open('IRSystem/ir_' + word_reduction_type + '_with_stopwords.pkl', 'rb') as handle:\n",
    "                ir = pickle.load(handle)\n",
    "            evaluation = evaluate(ir, queries_with_stopwords)\n",
    "            results = pd.concat([results, pd.DataFrame({'word_reduction_type': word_reduction_type, 'stopwords': 1, 'query_type': evaluation['query_type'], 'precision': evaluation['precision'], 'recall': evaluation['recall']})], ignore_index=True)\n",
    "            del ir # Free memory\n",
    "        else:\n",
    "            with open('IRSystem/ir_' + word_reduction_type + '_without_stopwords.pkl', 'rb') as handle:\n",
    "                ir = pickle.load(handle)\n",
    "            evaluation = evaluate(ir, queries_without_stopwords)\n",
    "            results = pd.concat([results, pd.DataFrame({'word_reduction_type': word_reduction_type, 'stopwords': 0, 'query_type': evaluation['query_type'], 'precision': evaluation['precision'], 'recall': evaluation['recall']})], ignore_index=True)\n",
    "            del ir # Free memory\n",
    "\n",
    "# Save results on disk\n",
    "results.to_csv('results/evaluation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
